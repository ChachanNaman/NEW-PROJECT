# Flume configuration for ingesting user activity data
# Store data in HDFS for batch processing

agent.sources = r1
agent.sinks = k1
agent.channels = c1

# Source: Monitor log files for user activities
agent.sources.r1.type = exec
agent.sources.r1.command = tail -F /var/log/recohub/activities.log
agent.sources.r1.channels = c1

# Channel: Memory channel for buffering
agent.channels.c1.type = memory
agent.channels.c1.capacity = 10000
agent.channels.c1.transactionCapacity = 1000

# Sink: Write to HDFS
agent.sinks.k1.type = hdfs
agent.sinks.k1.hdfs.path = hdfs://localhost:9000/user/recohub/flume/activities/%Y/%m/%d
agent.sinks.k1.hdfs.fileType = DataStream
agent.sinks.k1.hdfs.writeFormat = Text
agent.sinks.k1.hdfs.rollInterval = 3600
agent.sinks.k1.hdfs.rollSize = 134217728
agent.sinks.k1.hdfs.rollCount = 0
agent.sinks.k1.hdfs.useLocalTimeStamp = true
agent.sinks.k1.channel = c1

# Alternative: Send to Kafka instead of HDFS
# agent.sinks.k1.type = org.apache.flume.plugins.KafkaSink
# agent.sinks.k1.kafka.bootstrap.servers = localhost:9092
# agent.sinks.k1.kafka.topic = user-activities
# agent.sinks.k1.kafka.flumeBatchSize = 20
# agent.sinks.k1.kafka.flumeBatchCount = 10

